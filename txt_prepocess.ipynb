{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "import PyPDF2 # for reading pdf files\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('wordnet')\n",
    "import PyPDF2\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #stemmer = nltk.PorterStemmer()\n",
    "    # Convert text to lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())  \n",
    "    # Remove stopwords and lemmatize\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stopwords]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Remove punctuation\n",
    "    # stemmed_tokens = [nltk.PorterStemmer().stem(token) for token in tokens]\n",
    "    lemmatized_tokens_no_punct = [''.join(c for c in token if c not in string.punctuation) for token in lemmatized_tokens]\n",
    "    stemmed_tokens_no_punct = [token for token in lemmatized_tokens_no_punct if token]\n",
    "    return ' '.join(lemmatized_tokens_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world test sentence contains punctuation comma period exclamation mark question mark function work correctly'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Hello, world! This is a test sentence. It contains punctuation, such as commas, periods, exclamation marks, and question marks. Does the function work correctly?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, n):\n",
    "    preprocessed_text = preprocess(text)\n",
    "    \n",
    "    # I can use pre trained model like BERT, GPT-2, etc. for keyword extraction here later \n",
    "    vectorizer = TfidfVectorizer() #ngram_range=(1, 2) for bigrams \n",
    "    # Fit and transform the preprocessed text\n",
    "    tfidf_matrix = vectorizer.fit_transform([preprocessed_text])\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    #Get the TF-IDF scores for each feature\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]#.flatten()\n",
    "    # Create a dictionary of feature names and their corresponding TF-IDF scores\n",
    "    tfidf_dict = dict(zip(feature_names, tfidf_scores))\n",
    "    #sort the dictionary by TF-IDF scores in descending order\n",
    "    sorted_tfidf_dict = dict(sorted(tfidf_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    keywords = list(sorted_tfidf_dict.keys())[:n]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anime', '2013', 'attack', 'first', 'hit']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords(\"Anime is getting very popular in India. The rise of Anime got at a very high volume when the COVID virus hit and the lockdown started. Though, I have been watching it since 2013 with my first favourite anime being Attack on Titan. Attack on Titan was first aired on 2013 and it was massive hit as an anime and the community really like it.\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        pdf_file_obj = open(file_path, 'rb')\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "        text = \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "        pdf_file_obj.close()\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = Document(file_path)\n",
    "        text = \" \".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    else:\n",
    "        print(\"Unsupported file type\")\n",
    "        text = \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anime', '2013', 'attack', 'first', 'hit']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/Naman/Downloads/Anime.txt'  \n",
    "text = read_file(file_path)\n",
    "keywords = extract_keywords(text, n=5)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023', 'github', 'learning', 'model', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/Naman/Documents/Naman_Singh_Resume.pdf'  \n",
    "text = read_file(file_path)\n",
    "keywords = extract_keywords(text,5)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summarization', 'user', 'system', 'using', 'knee']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/Naman/Downloads/Final_project_report.docx'  \n",
    "text = read_file(file_path)\n",
    "keywords = extract_keywords(text,5)\n",
    "print(keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
